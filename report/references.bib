
@article{gardner_gpytorch_2021,
	title = {{GPyTorch}: Blackbox Matrix-Matrix Gaussian Process Inference with {GPU} Acceleration},
	url = {http://arxiv.org/abs/1809.11165},
	shorttitle = {{GPyTorch}},
	abstract = {Despite advances in scalable models, the inference tools used for Gaussian processes ({GPs}) have yet to fully capitalize on developments in computing hardware. We present an efﬁcient and general approach to {GP} inference based on Blackbox Matrix-Matrix multiplication ({BBMM}). {BBMM} inference uses a modiﬁed batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. {BBMM} reduces the asymptotic complexity of exact {GP} inference from O(n3) to O(n2). Adapting this algorithm to scalable approximations and complex {GP} models simply requires a routine for efﬁcient matrix-matrix multiplication with the kernel and its derivative. In addition, {BBMM} uses a specialized preconditioner to substantially speed up convergence. In experiments we show that {BBMM} effectively uses {GPU} hardware to dramatically accelerate both exact {GP} inference and scalable approximations. Additionally, we provide {GPyTorch}, a software platform for scalable {GP} inference via {BBMM}, built on {PyTorch}.},
	journaltitle = {{arXiv}:1809.11165 [cs, stat]},
	author = {Gardner, Jacob R. and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
	date = {2021-06-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1809.11165},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}


@article{harbrecht_low-rank_2012,
	title = {On the low-rank approximation by the pivoted Cholesky decomposition},
	volume = {62},
	issn = {0168-9274},
	url = {https://www.sciencedirect.com/science/article/pii/S0168927411001814},
	doi = {10.1016/j.apnum.2011.10.001},
	series = {Third Chilean Workshop on Numerical Analysis of Partial Differential Equations ({WONAPDE} 2010)},
	abstract = {The present paper is dedicated to the application of the pivoted Cholesky decomposition to compute low-rank approximations of dense, positive semi-definite matrices. The resulting truncation error is rigorously controlled in terms of the trace norm. Exponential convergence rates are proved under the assumption that the eigenvalues of the matrix under consideration exhibit a sufficiently fast exponential decay. By numerical experiments it is demonstrated that the pivoted Cholesky decomposition leads to very efficient algorithms to separate the variables of bi-variate functions.},
	pages = {428--440},
	number = {4},
	journaltitle = {Applied Numerical Mathematics},
	shortjournal = {Applied Numerical Mathematics},
	author = {Harbrecht, Helmut and Peters, Michael and Schneider, Reinhold},
	date = {2012-04-01},
	langid = {english},
	keywords = {Cholesky decomposition, Low-rank approximation},
}


@article{henderson_deriving_1981,
	title = {On Deriving the Inverse of a Sum of Matrices},
	volume = {23},
	issn = {0036-1445},
	url = {https://www.jstor.org/stable/2029838},
	abstract = {Available expressions are reviewed and new ones derived for the inverse of the sum of two matrices, one of them being nonsingular. Particular attention is given to (A + {UBV})-1, where A is nonsingular and U, B and V may be rectangular; generalized inverses of A + {UBV} are also considered. Several statistical applications are discussed.},
	pages = {53--60},
	number = {1},
	journaltitle = {{SIAM} Review},
	author = {Henderson, H. V. and Searle, S. R.},
	date = {1981},
	note = {Publisher: Society for Industrial and Applied Mathematics},
}

@article{ubaru_fast_2017,
	title = {Fast Estimation of tr(f(A)) via Stochastic Lanczos Quadrature},
	volume = {38},
	issn = {0895-4798, 1095-7162},
	url = {https://epubs.siam.org/doi/10.1137/16M1104974},
	doi = {10.1137/16M1104974},
	pages = {1075--1099},
	number = {4},
	journaltitle = {{SIAM} Journal on Matrix Analysis and Applications},
	shortjournal = {{SIAM} J. Matrix Anal. \& Appl.},
	author = {Ubaru, Shashanka and Chen, Jie and Saad, Yousef},
	date = {2017-01},
	langid = {english},
	note = {Number: 4},
}

@article{roosta-khorasani_improved_2015,
	title = {Improved bounds on sample size for implicit matrix trace estimators},
	volume = {15},
	issn = {1615-3375, 1615-3383},
	url = {http://arxiv.org/abs/1308.2475},
	doi = {10.1007/s10208-014-9220-1},
	abstract = {This article is concerned with Monte-Carlo methods for the estimation of the trace of an implicitly given matrix A whose information is only available through matrix-vector products. Such a method approximates the trace by an average of N expressions of the form wt(Aw), with random vectors w drawn from an appropriate distribution. We prove, discuss and experiment with bounds on the number of realizations N required in order to guarantee a probabilistic bound on the relative error of the trace estimation upon employing Rademacher (Hutchinson), Gaussian and uniform unit vector (with and without replacement) probability distributions.},
	pages = {1187--1212},
	number = {5},
	journaltitle = {Foundations of Computational Mathematics},
	shortjournal = {Found Comput Math},
	author = {Roosta-Khorasani, Farbod and Ascher, Uri},
	date = {2015-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1308.2475},
	note = {Number: 5},
	keywords = {Mathematics - Numerical Analysis, 65C20, 65C05, 68W20},
}

@article{cortinovis_randomized_2021,
	title = {On randomized trace estimates for indefinite matrices with an application to determinants},
	url = {http://arxiv.org/abs/2005.10009},
	abstract = {Randomized trace estimation is a popular and well studied technique that approximates the trace of a large-scale matrix \$B\$ by computing the average of \$x{\textasciicircum}T Bx\$ for many samples of a random vector \$X\$. Often, \$B\$ is symmetric positive definite ({SPD}) but a number of applications give rise to indefinite \$B\$. Most notably, this is the case for log-determinant estimation, a task that features prominently in statistical learning, for instance in maximum likelihood estimation for Gaussian process regression. The analysis of randomized trace estimates, including tail bounds, has mostly focused on the {SPD} case. In this work, we derive new tail bounds for randomized trace estimates applied to indefinite \$B\$ with Rademacher or Gaussian random vectors. These bounds significantly improve existing results for indefinite \$B\$, reducing the the number of required samples by a factor \$n\$ or even more, where \$n\$ is the size of \$B\$. Even for an {SPD} matrix, our work improves an existing result by Roosta-Khorasani and Ascher for Rademacher vectors. This work also analyzes the combination of randomized trace estimates with the Lanczos method for approximating the trace of \$f(A)\$. Particular attention is paid to the matrix logarithm, which is needed for log-determinant estimation. We improve and extend an existing result, to not only cover Rademacher but also Gaussian random vectors.},
	journaltitle = {{arXiv}:2005.10009 [cs, math]},
	author = {Cortinovis, Alice and Kressner, Daniel},
	date = {2021-05-25},
	eprinttype = {arxiv},
	eprint = {2005.10009},
	keywords = {Mathematics - Numerical Analysis, 65C05 (Primary) 65F40, 65F60, 68W20, 60E15 (Secondary)},
}

@book{golub_matrices_2010,
	location = {Princeton, N.J},
	title = {Matrices, moments and quadrature with applications},
	isbn = {978-0-691-14341-5},
	series = {Princeton series in applied mathematics},
	publisher = {Princeton University Press},
	author = {Golub, Gene H.},
	editora = {Golub, Gene Howard and Golub, Gene Howard and Meurant, Gérard and Meurant, Gérard A. and Meurant, Gérard A.},
	editoratype = {collaborator},
	date = {2010},
	keywords = {Analyse numérique, {EIGENWERTE} {UND} {EIGENVEKTOREN} {VON} {MATRIZEN} ({NUMERISCHE} {MATHEMATIK}), {LANCZOSALGORITHMUS} ({NUMERISCHE} {MATHEMATIK}), Matrices, Matrix (Math.), {MATRIZENINVERSION} ({NUMERISCHE} {MATHEMATIK}), {METHODE} {DER} {KONJUGIERTEN} {GRADIENTEN} ({NUMERISCHE} {MATHEMATIK}), {METHODEN} {DER} {KLEINSTEN} {QUADRATE} ({NUMERISCHE} {MATHEMATIK}), Numerical analysis, Numerische Mathematik, {POLYNOMREIHEN} + {ORTHOGONALE} {POLYNOME} ({ANALYSIS} {EINER} {KOMPLEXEN} {VARIABLEN}), {QUADRATURFORMELN} ({NUMERISCHE} {MATHEMATIK}), {SPEZIELLE} {TYPEN} {VON} {MATRIZEN} ({ALGEBRA})},
}

@book{saad_iterative_2003,
	location = {Philadelphia},
	edition = {2nd ed.},
	title = {Iterative methods for sparse linear systems},
	isbn = {978-0-89871-534-7},
	abstract = {Increased participation in world trade is conventionally seen as the key to economic growth and development. Yet, as this book shows through its detailed examination of contemporary world trade patterns, while developing country exports have grown faster than the world average, the rich countries have meanwhile increased their share in world manufacturing valued added. This poses the vitally important policy challenge of what poor countries, confronted by the vigorous expansion of their foreign trade but no comparable rise in income, should do. Primary commodity prices have collapsed in value, and there is a real danger that the terms of trade for their exports of manufactured goods may do the same. The key challenge confronting poor countries today is not more trade liberalization on their part, but how to improve the terms of their participation in world trade and to increase the still limited and unstable benefits they derive from it.},
	pagetotal = {xiii+528},
	publisher = {{SIAM}},
	author = {Saad, Youcef and Saad, Yousef},
	date = {2003},
	keywords = {Differential equations, Partial, Iterative methods Mathematics, {ITERATIVE} {VERFAHREN} ({NUMERISCHE} {MATHEMATIK}), Numerical solutions, {PARALLELE} {NUMERIK} ({NUMERISCHE} {MATHEMATIK}), {PARTIELLE} {DIFFERENTIALGLEICHUNGEN} ({NUMERISCHE} {MATHEMATIK}), {SCHWACHBESETZTE} {MATRIZEN} ({NUMERISCHE} {MATHEMATIK}), Sparse matrices},
}


@article{braun_accurate_2006,
	title = {Accurate Error Bounds for the Eigenvalues of the Kernel Matrix},
	volume = {7},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v7/braun06a.html},
	abstract = {The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to infinity. We derive probabilistic finite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reflecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a significant improvement over existing non-scaling bounds.},
	pages = {2303--2328},
	number = {82},
	journaltitle = {Journal of Machine Learning Research},
	author = {Braun, Mikio L.},
	date = {2006},
	file = {Full Text PDF:/home/maousi/Zotero/storage/TEL8E52J/Braun - 2006 - Accurate Error Bounds for the Eigenvalues of the K.pdf:application/pdf},
}

@book{rasmussen_gaussian_2005,
	location = {Cambridge, {MA}, {USA}},
	title = {Gaussian Processes for Machine Learning},
	isbn = {978-0-262-18253-9},
	series = {Adaptive Computation and Machine Learning series},
	abstract = {A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines.},
	pagetotal = {272},
	publisher = {{MIT} Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	editorb = {Bach, Francis},
	editorbtype = {redactor},
	date = {2005-11-23},
	langid = {english},
}


@book{gil_numerical_2007,
	title = {Numerical Methods for Special Functions},
	isbn = {978-0-89871-634-4},
	abstract = {Probably, the most extended (pseudo) definition of the set of functions known as “special functions” refers to those mathematical functions which are widely used in scientific and technical applications, and of which many useful properties are known. These functions are typically used in two related contexts:
1. as a way of obtaining simple closed formulas and other analytical properties of solutions of problems from pure and applied mathematics, statistics, physics, and engineering;
2. as a way of understanding the nature of the solutions of these problems, and for obtaining numerical results from the representations of the functions.
Our book is intended to provide assistance when a researcher or a student needs to get the numbers from analytical formulas containing special functions. This book should be useful for those who need to compute a function by their own means, or for those who want to know more about the numerical methods behind the available algorithms. Our main purpose is to provide a guide of available methods for computations and when to use them. Also, because of the large variety of numerical methods that are available for computing special functions, we expect that a broader “numerical audience” will be interested in many of the topics discussed (particularly in the first part of the book). Several levels of reading are possible in this book and most of the chapters start with basic principles. Examples are given to illustrate the use of the methods, pseudoalgorithms are given to describe technical details, and published algorithms for computing a selection of functions are described as practical illustrations for the basic methods of this book.
The presentation of the topics is organized in four parts: Basic Methods, Further Tools and Methods, Related Topics and Examples, and Software. The first part (Basic Methods) describes a set of methods which, in our experience, are the most popular and important ones for computing special functions. This includes convergent and divergent series, Chebyshev expansions, linear recurrence relations, and quadrature methods. These basic chapters are mostly self-contained and start from first principles. We expect that many of the contents are appropriate for advanced numerical analysis courses (parts of the chapters are in fact based on classroom notes); however, because the main focus is on special functions, detailed examples of application are also provided.
The second part of the book (Further Tools and Methods) contains a set of important methods for computing special functions which, however, are probably not so well known as the basic methods (at least for readers who are not very familiar with special functions).},
	author = {Gil, Amparo and Segura, Javier and Temme, Nico},
	date = {2007-01-01},
	doi = {10.1137/1.9780898717822},
	file = {Full Text PDF:C\:\\Users\\mzell\\Zotero\\storage\\QYC5923Q\\Gil et al. - 2007 - Numerical Methods for Special Functions.pdf:application/pdf},
}


@article{elliott_error_1987,
	title = {Error of truncated Chebyshev series and other near minimax polynomial approximations},
	volume = {50},
	issn = {00219045},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0021904587900657},
	doi = {10.1016/0021-9045(87)90065-7},
	pages = {49--57},
	number = {1},
	journaltitle = {Journal of Approximation Theory},
	shortjournal = {Journal of Approximation Theory},
	author = {Elliott, D. and Paget, D.F. and Phillips, G.M. and Taylor, P.J.},
	date = {1987-05},
	langid = {english},
	file = {Elliott et al. - 1987 - Error of truncated Chebyshev series and other near.pdf:C\:\\Users\\mzell\\Zotero\\storage\\8BM3Z8UN\\Elliott et al. - 1987 - Error of truncated Chebyshev series and other near.pdf:application/pdf},
}


@book{tong_multivariate_1990,
	location = {New York Berlin Heidelberg},
	title = {The multivariate normal distribution},
	isbn = {978-1-4613-9655-0 978-1-4613-9657-4 978-0-387-97062-2 978-3-540-97062-0},
	series = {Springer series in statistics},
	pagetotal = {271},
	publisher = {Springer},
	author = {Tong, Yung L.},
	date = {1990},
	file = {Table of Contents PDF:C\:\\Users\\mzell\\Zotero\\storage\\XBK2WFMB\\Tong - 1990 - The multivariate normal distribution.pdf:application/pdf},
}


@book{abramowitz_handbook_1972,
	title = {Handbook of mathematical functions with formulas, graphs, and mathematical tables},
	isbn = {978-0-486-61272-0 978-0-471-80007-1},
	url = {http://archive.org/details/handbookofmathe000abra},
	abstract = {Originally published by National Bureau of Standards as part of the series Selected government publications; Includes bibliographies and indexes},
	pagetotal = {1078},
	publisher = {New York : Wiley},
	author = {Abramowitz, Milton and Stegun, Irene A. and {United States. National Bureau of Standards}},
	editora = {{Internet Archive}},
	editoratype = {collaborator},
	urldate = {2022-04-13},
	date = {1972},
	keywords = {Functions},
}


@article{borovitskiy_matern_nodate,
	title = {Matérn Gaussian processes on Riemannian manifolds},
	abstract = {Gaussian processes are an effective model class for learning unknown functions, particularly in settings where accurately representing predictive uncertainty is of key importance. Motivated by applications in the physical sciences, the widelyused Mate´rn class of Gaussian processes has recently been generalized to model functions whose domains are Riemannian manifolds, by re-expressing said processes as solutions of stochastic partial differential equations. In this work, we propose techniques for computing the kernels of these processes on compact Riemannian manifolds via spectral theory of the Laplace–Beltrami operator in a fully constructive manner, thereby allowing them to be trained via standard scalable techniques such as inducing point methods. We also extend the generalization from the Mate´rn to the widely-used squared exponential Gaussian process. By allowing Riemannian Mate´rn Gaussian processes to be trained using well-understood techniques, our work enables their use in mini-batch, online, and non-conjugate settings, and makes them more accessible to machine learning practitioners.},
	pages = {12},
	author = {Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc Peter},
	langid = {english},
	file = {Borovitskiy et al. - Mate´rn Gaussian processes on Riemannian manifolds.pdf:C\:\\Users\\Matthias\\Zotero\\storage\\SU9CPP2R\\Borovitskiy et al. - Mate´rn Gaussian processes on Riemannian manifolds.pdf:application/pdf},
}


@article{genton_classes_2002,
	title = {Classes of kernels for machine learning: a statistics perspective},
	volume = {2},
	issn = {1532-4435},
	shorttitle = {Classes of kernels for machine learning},
	abstract = {In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.},
	pages = {299--312},
	journaltitle = {The Journal of Machine Learning Research},
	shortjournal = {J. Mach. Learn. Res.},
	author = {Genton, Marc G.},
	date = {2002-03-01},
	file = {Full Text PDF:C\:\\Users\\Matthias\\Zotero\\storage\\UHLSCSXW\\Genton - 2002 - Classes of kernels for machine learning a statist.pdf:application/pdf},
}


@book{banerjee_parallel_2013,
	title = {Parallel inversion of huge covariance matrices},
	abstract = {An extremely common bottleneck encountered in statistical learning algorithms is inversion of huge covariance matrices, examples being in evaluating Gaussian likelihoods for a large number of data points. We propose general parallel algorithms for inverting positive definite matrices, which are nearly rank deficient. Such matrix inversions are needed in Gaussian process computations, among other settings, and remain a bottleneck even with the increasing literature on low rank approximations. We propose a general class of algorithms for parallelizing computations to dramatically speed up computation time by orders of magnitude exploiting multicore architectures. We implement our algorithm on a cloud computing platform, providing pseudo and actual code. The algorithm can be easily implemented on any multicore parallel computing resource. Some illustrations are provided to give a flavor for the gains and what becomes possible in freeing up this bottleneck.},
	author = {Banerjee, Anjishnu and Vogelstein, Joshua and Dunson, David},
	date = {2013-12-06},
	file = {Full Text PDF:C\:\\Users\\mzell\\Zotero\\storage\\TJSJ8TS7\\Banerjee et al. - 2013 - Parallel inversion of huge covariance matrices.pdf:application/pdf},
}

@article{pan_learning_2011,
	title = {Learning low-rank Mercer kernels with fast-decaying spectrum},
	volume = {74},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S092523121100302X},
	doi = {10.1016/j.neucom.2011.04.021},
	abstract = {Low-rank representations have received a lot of interest in the application of kernel-based methods. However, these methods made an assumption that the spectrum of the Gaussian or polynomial kernels decays rapidly. This is not always true and its violation may result in performance degradation. In this paper, we propose an effective technique for learning low-rank Mercer kernels ({LMK}) with fastdecaying spectrum. What distinguishes our kernels from other classical kernels (Gaussian and polynomial kernels) is that the proposed always yields low-rank Gram matrices whose spectrum decays rapidly, no matter what distribution the data are. Furthermore, the {LMK} can control the decay rate. Thus, our kernels can prevent performance degradation while using the low-rank approximations. Our algorithm has favorable in scalability—it is linear in the number of data points and quadratic in the rank of the Gram matrix. Empirical results demonstrate that the proposed method learns fast-decaying spectrum and signiﬁcantly improves the performance.},
	pages = {3028--3035},
	number = {17},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Pan, Binbin and Lai, Jianhuang and Yuen, Pong C.},
	date = {2011-10},
	langid = {english},
	file = {Pan et al. - 2011 - Learning low-rank Mercer kernels with fast-decayin.pdf:C\:\\Users\\mzell\\Zotero\\storage\\K2NDQHX8\\Pan et al. - 2011 - Learning low-rank Mercer kernels with fast-decayin.pdf:application/pdf},
}



@article{bach_kernel_2002,
	title = {Kernel Independent Component Analysis},
	volume = {3},
	issn = {{ISSN} 1533-7928},
	url = {https://www.jmlr.org/papers/v3/bach02a},
	abstract = {We present a class of algorithms for independent component analysis ({ICA}) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space. On the one hand, we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of statistical dependence. On the other hand, building on recent developments in kernel methods, we show that these criteria and their derivatives can be computed efficiently. Minimizing these criteria leads to flexible and robust algorithms for {ICA}. We illustrate with simulations involving a wide variety of source distributions, showing that our algorithms outperform many of the presently known algorithms.},
	pages = {1--48},
	issue = {Jul},
	journaltitle = {Journal of Machine Learning Research},
	author = {Bach, Francis R. and Jordan, Michael I.},
	date = {2002},
	file = {Full Text PDF:C\:\\Users\\mzell\\Zotero\\storage\\I4FWXF77\\Bach and Jordan - 2002 - Kernel Independent Component Analysis.pdf:application/pdf},
}


@book{baker_numerical_1977,
	location = {Oxford},
	title = {The numerical treatment of integral equations},
	isbn = {978-0-19-853406-8},
	series = {Monographs on numerical analysis},
	pagetotal = {xiv+1034},
	publisher = {Clarendon Press},
	author = {Baker, Christopher T. H.},
	date = {1977},
	keywords = {Integral equations, Integralgleichung, {INTEGRALGLEICHUNGEN} ({NUMERISCHE} {MATHEMATIK}), Numerical solutions, Numerische Mathematik, Numerisches Verfahren},
}

@article{todor_robust_2006,
	title = {Robust Eigenvalue Computation for Smoothing Operators},
	volume = {44},
	issn = {0036-1429, 1095-7170},
	url = {http://epubs.siam.org/doi/10.1137/040616449},
	doi = {10.1137/040616449},
	abstract = {Robust quasi-relative Galerkin discretization error estimates are derived for the eigenvalue problem associated to a nonnegative compact operator K acting in a Hilbert space. Trace discretization error estimates for arbitrarily small positive powers of K are obtained as a consequence. Coupled with bounds on eigenfunction oscillations, the results are then applied to the case of an integral operator with (piecewise) smooth kernel K on a bounded domain and in the context of the h ﬁnite element method.},
	pages = {865--878},
	number = {2},
	journaltitle = {{SIAM} Journal on Numerical Analysis},
	shortjournal = {{SIAM} J. Numer. Anal.},
	author = {Todor, Radu Alexandru},
	date = {2006-01},
	langid = {english},
	file = {Todor - 2006 - Robust Eigenvalue Computation for Smoothing Operat.pdf:/home/maousi/Zotero/storage/BES3KFET/Todor - 2006 - Robust Eigenvalue Computation for Smoothing Operat.pdf:application/pdf},
}

@article{schwab_karhunenloeve_2006,
	title = {Karhunen–Loève approximation of random fields by generalized fast multipole methods},
	volume = {217},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999106000349},
	doi = {10.1016/j.jcp.2006.01.048},
	series = {Uncertainty Quantification in Simulation Science},
	abstract = {{KL} approximation of a possibly instationary random field a(ω,x)∈L2(Ω,{dP};L∞(D)) subject to prescribed meanfield Ea(x)=∫Ωa(ω,x){dP}(ω) and covariance Va(x,x′)=∫Ω(a(ω,x)-Ea(x))(a(ω,x′)-Ea(x′)){dP}(ω) in a polyhedral domain D⊂Rd is analyzed. We show how for stationary covariances Va(x,x′)=ga({\textbar}x−x′{\textbar}) with ga(z) analytic outside of z=0, an M-term approximate {KL}-expansion {aM}(ω,x) of a(ω,x) can be computed in log-linear complexity. The approach applies in arbitrary domains D and for nonseparable covariances Ca. It involves Galerkin approximation of the {KL} eigenvalue problem by discontinuous finite elements of degree p⩾0 on a quasiuniform, possibly unstructured mesh of width h in D, plus a generalized fast multipole accelerated Krylov-Eigensolver. The approximate {KL}-expansion {aM}(x,ω) of a(x,ω) has accuracy O(exp(−{bM}1/d)) if ga is analytic at z=0 and accuracy O(M−k/d) if ga is Ck at zero. It is obtained in O({MN}({logN})b) operations where N=O(h−d).},
	pages = {100--122},
	number = {1},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Schwab, Christoph and Todor, Radu Alexandru},
	date = {2006-09-01},
	langid = {english},
	file = {ScienceDirect Snapshot:/home/maousi/Zotero/storage/MTJJB52P/S0021999106000349.html:text/html},
}

@book{james_introduction_2021,
	location = {New York, {NY}},
	title = {An Introduction to Statistical Learning: with Applications in R},
	isbn = {978-1-07-161417-4 978-1-07-161418-1},
	url = {https://link.springer.com/10.1007/978-1-0716-1418-1},
	series = {Springer Texts in Statistics},
	shorttitle = {An Introduction to Statistical Learning},
	publisher = {Springer {US}},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-1-0716-1418-1},
	keywords = {data mining, inference, R, R software, statistical learning, supervised learning, unsupervised learning},
	file = {Full Text PDF:/home/maousi/Zotero/storage/KQ6LDTDR/James et al. - 2021 - An Introduction to Statistical Learning with Appl.pdf:application/pdf},
}

@book{golub_matrix_2013,
	location = {Baltimore},
	edition = {4th [rev.] ed.},
	title = {Matrix computations},
	isbn = {978-1-4214-0794-4},
	series = {Johns Hopkins studies in the mathematical sciences},
	pagetotal = {756},
	publisher = {Johns Hopkins University Press},
	author = {Golub, Gene H.},
	editora = {Golub, Gene Howard and Loan, Charles F. van and Van Loan, Charles F.},
	editoratype = {collaborator},
	date = {2013},
	keywords = {{ANALYTISCHE} {MATRIZEN} + {MATRIXFUNKTIONEN} ({ANALYSIS}), Data processing, {EIGENWERTE} {UND} {EIGENVEKTOREN} {VON} {MATRIZEN} ({NUMERISCHE} {MATHEMATIK}), Informatique, {LEHRBÜCHER} ({DOKUMENTENTYP}), {LINEARE} {GLEICHUNGSSYSTEME} ({NUMERISCHE} {MATHEMATIK}), Matrices, Matrix Mathematik, {MATRIZEN} {UND} {LINEARE} {ABBILDUNGEN} ({ALGEBRA}), Matrizenrechnung, Matrizentheorie, {METHODEN} {DER} {KLEINSTEN} {QUADRATE} ({NUMERISCHE} {MATHEMATIK}), Numerische Mathematik, {SCHWACHBESETZTE} {MATRIZEN} ({NUMERISCHE} {MATHEMATIK}), {SPEZIELLE} {TYPEN} {VON} {MATRIZEN} ({ALGEBRA}), Systèmes linéaires},
}

@article{higham_survey_1987,
	title = {A Survey of Condition Number Estimation for Triangular Matrices},
	volume = {29},
	issn = {0036-1445},
	url = {http://siamdl.aip.org/sirev},
	abstract = {We survey and compare a wide variety of techniques for estimating the condition number
of a triangular matrix, and make recommendations concerning the use of the estimates in applications.
Each of the methods is shown to bound the condition number; the bounds can broadly be categorised as
upper bounds from matrix theory and lower bounds from heuristic or probabilistic algorithms. For each
bound we examine by how much, at worst, it can overestimate or underestimate the condition number.
Numerical experiments are presented in order to illustrate and compare the practical performance of the
condition estimators.},
	pages = {575--596},
	number = {4},
	journaltitle = {{SIAM} Review},
	author = {Higham, Nicholas J.},
	date = {1987},
	langid = {english},
	note = {Number: 4},
	file = {Full Text PDF:/home/maousi/Zotero/storage/GKB47FPM/Higham - 1987 - A Survey of Condition Number Estimation for Triang.pdf:application/pdf;Snapshot:/home/maousi/Zotero/storage/UY27GNRY/695.html:text/html},
}

@article{schulz_tutorial_2018,
	title = {A tutorial on Gaussian process regression: Modelling, exploring, and exploiting functions},
	volume = {85},
	issn = {0022-2496},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249617302158},
	doi = {10.1016/j.jmp.2018.03.001},
	shorttitle = {A tutorial on Gaussian process regression},
	abstract = {This tutorial introduces the reader to Gaussian process regression as an expressive tool to model, actively explore and exploit unknown functions. Gaussian process regression is a powerful, non-parametric Bayesian approach towards regression problems that can be utilized in exploration and exploitation scenarios. This tutorial aims to provide an accessible introduction to these techniques. We will introduce Gaussian processes which generate distributions over functions used for Bayesian non-parametric regression, and demonstrate their use in applications and didactic examples including simple regression problems, a demonstration of kernel-encoded prior assumptions and compositions, a pure exploration scenario within an optimal design framework, and a bandit-like exploration–exploitation scenario where the goal is to recommend movies. Beyond that, we describe a situation modelling risk-averse exploration in which an additional constraint (not to sample below a certain threshold) needs to be accounted for. Lastly, we summarize recent psychological experiments utilizing Gaussian processes. Software and literature pointers are also provided.},
	pages = {1--16},
	journaltitle = {Journal of Mathematical Psychology},
	shortjournal = {Journal of Mathematical Psychology},
	author = {Schulz, Eric and Speekenbrink, Maarten and Krause, Andreas},
	date = {2018-08-01},
	langid = {english},
	keywords = {Active learning, Bandit problems, Exploration–exploitation, Gaussian process regression},
	file = {ScienceDirect Snapshot:/home/maousi/Zotero/storage/B9GYC6R4/S0022249617302158.html:text/html;Submitted Version:/home/maousi/Zotero/storage/AFQLCVKU/Schulz et al. - 2018 - A tutorial on Gaussian process regression Modelli.pdf:application/pdf},
}

@misc{liu_when_2019,
	title = {When Gaussian Process Meets Big Data: A Review of Scalable {GPs}},
	url = {http://arxiv.org/abs/1807.01065},
	shorttitle = {When Gaussian Process Meets Big Data},
	abstract = {The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process ({GP}) regression, a well-known non-parametric and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable {GPs} have been presented. But they have not yet been comprehensively reviewed and analyzed in order to be well understood by both academia and industry. The review of scalable {GPs} in the {GP} community is timely and important due to the explosion of data size. To this end, this paper is devoted to the review on state-of-the-art scalable {GPs} involving two main categories: global approximations which distillate the entire data and local approximations which divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations which modify the prior but perform exact inference, posterior approximations which retain exact prior but perform approximate inference, and structured sparse approximations which exploit speciﬁc structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable {GPs} are reviewed. Finally, the extensions and open issues regarding the implementation of scalable {GPs} in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.},
	publisher = {{arXiv}},
	author = {Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
	date = {2019-04-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.01065 [cs, stat]},
	note = {Number: {arXiv}:1807.01065},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Liu et al. - 2019 - When Gaussian Process Meets Big Data A Review of .pdf:/home/maousi/Zotero/storage/J45YMMBR/Liu et al. - 2019 - When Gaussian Process Meets Big Data A Review of .pdf:application/pdf},
}

@article{deringer_gaussian_2021,
	title = {Gaussian Process Regression for Materials and Molecules},
	volume = {121},
	issn = {0009-2665},
	url = {https://doi.org/10.1021/acs.chemrev.1c00022},
	doi = {10.1021/acs.chemrev.1c00022},
	abstract = {We provide an introduction to Gaussian process regression ({GPR}) machine-learning methods in computational materials science and chemistry. The focus of the present review is on the regression of atomistic properties: in particular, on the construction of interatomic potentials, or force fields, in the Gaussian Approximation Potential ({GAP}) framework; beyond this, we also discuss the fitting of arbitrary scalar, vectorial, and tensorial quantities. Methodological aspects of reference data generation, representation, and regression, as well as the question of how a data-driven model may be validated, are reviewed and critically discussed. A survey of applications to a variety of research questions in chemistry and materials science illustrates the rapid growth in the field. A vision is outlined for the development of the methodology in the years to come.},
	pages = {10073--10141},
	number = {16},
	journaltitle = {Chemical Reviews},
	shortjournal = {Chem. Rev.},
	author = {Deringer, Volker L. and Bartók, Albert P. and Bernstein, Noam and Wilkins, David M. and Ceriotti, Michele and Csányi, Gábor},
	date = {2021-08-25},
	note = {Publisher: American Chemical Society},
	file = {Full Text PDF:/home/maousi/Zotero/storage/VEDZMZYV/Deringer et al. - 2021 - Gaussian Process Regression for Materials and Mole.pdf:application/pdf},
}

@misc{belyaev_exact_2014,
	title = {Exact Inference for Gaussian Process Regression in case of Big Data with the Cartesian Product Structure},
	url = {http://arxiv.org/abs/1403.6573},
	abstract = {Approximation algorithms are widely used in many engineering problems. To obtain a data set for approximation a factorial design of experiments is often used. In such case the size of the data set can be very large. Therefore, one of the most popular algorithms for approximation —Gaussian Process regression — can be hardly applied due to its computational complexity. In this paper a new approach for Gaussian Process regression in case of factorial design of experiments is proposed. It allows to efﬁciently compute exact inference and handle large multidimensional data sets. The proposed algorithm provides fast and accurate approximation and also handles anisotropic data.},
	publisher = {{arXiv}},
	author = {Belyaev, Mikhail and Burnaev, Evgeny and Kapushev, Yermek},
	date = {2014-07-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1403.6573 [math, stat]},
	note = {Number: {arXiv}:1403.6573},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology},
	file = {Belyaev et al. - 2014 - Exact Inference for Gaussian Process Regression in.pdf:/home/maousi/Zotero/storage/JICI6A7T/Belyaev et al. - 2014 - Exact Inference for Gaussian Process Regression in.pdf:application/pdf},
}

@article{seeger_gaussian_2004,
	title = {Gaussian processes for machine learning},
	volume = {14},
	issn = {0129-0657},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0129065704001899},
	doi = {10.1142/S0129065704001899},
	abstract = {Gaussian processes ({GPs}) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. {GPs} have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated.

Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on {GPs} is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of {GPs} relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
	pages = {69--106},
	number = {2},
	journaltitle = {International Journal of Neural Systems},
	shortjournal = {Int. J. Neur. Syst.},
	author = {Seeger, Matthias},
	date = {2004-04},
	note = {Publisher: World Scientific Publishing Co.},
	keywords = {Bayesian inference, Gaussian processes, Kernel methods, nonparametric statistics},
	file = {Submitted Version:/home/maousi/Zotero/storage/XAWVH69I/Seeger - 2004 - Gaussian processes for machine learning.pdf:application/pdf},
}

@inproceedings{gorbach_model_2017,
	location = {Cham},
	title = {Model Selection for Gaussian Process Regression},
	isbn = {978-3-319-66709-6},
	doi = {10.1007/978-3-319-66709-6_25},
	series = {Lecture Notes in Computer Science},
	abstract = {Gaussian processes are powerful tools since they can model non-linear dependencies between inputs, while remaining analytically tractable. A Gaussian process is characterized by a mean function and a covariance function (kernel), which are determined by a model selection criterion. The functions to be compared do not just differ in their parametrization but in their fundamental structure. It is often not clear which function structure to choose, for instance to decide between a squared exponential and a rational quadratic kernel. Based on the principle of posterior agreement, we develop a general framework for model selection to rank kernels for Gaussian process regression and compare it with maximum evidence (also called marginal likelihood) and leave-one-out cross-validation. Given the disagreement between current state-of-the-art methods in our experiments, we show the difficulty of model selection and the need for an information-theoretic approach.},
	pages = {306--318},
	booktitle = {Pattern Recognition},
	publisher = {Springer International Publishing},
	author = {Gorbach, Nico S. and Bian, Andrew An and Fischer, Benjamin and Bauer, Stefan and Buhmann, Joachim M.},
	editor = {Roth, Volker and Vetter, Thomas},
	date = {2017},
	langid = {english},
	file = {Full Text PDF:/home/maousi/Zotero/storage/HKMATH53/Gorbach et al. - 2017 - Model Selection for Gaussian Process Regression.pdf:application/pdf},
}

