
@article{gardner_gpytorch_2021,
	title = {{GPyTorch}: Blackbox Matrix-Matrix Gaussian Process Inference with {GPU} Acceleration},
	url = {http://arxiv.org/abs/1809.11165},
	shorttitle = {{GPyTorch}},
	abstract = {Despite advances in scalable models, the inference tools used for Gaussian processes ({GPs}) have yet to fully capitalize on developments in computing hardware. We present an efﬁcient and general approach to {GP} inference based on Blackbox Matrix-Matrix multiplication ({BBMM}). {BBMM} inference uses a modiﬁed batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. {BBMM} reduces the asymptotic complexity of exact {GP} inference from O(n3) to O(n2). Adapting this algorithm to scalable approximations and complex {GP} models simply requires a routine for efﬁcient matrix-matrix multiplication with the kernel and its derivative. In addition, {BBMM} uses a specialized preconditioner to substantially speed up convergence. In experiments we show that {BBMM} effectively uses {GPU} hardware to dramatically accelerate both exact {GP} inference and scalable approximations. Additionally, we provide {GPyTorch}, a software platform for scalable {GP} inference via {BBMM}, built on {PyTorch}.},
	journaltitle = {{arXiv}:1809.11165 [cs, stat]},
	author = {Gardner, Jacob R. and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
	urldate = {2022-03-16},
	date = {2021-06-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1809.11165},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}


@article{harbrecht_low-rank_2012,
	title = {On the low-rank approximation by the pivoted Cholesky decomposition},
	volume = {62},
	issn = {0168-9274},
	url = {https://www.sciencedirect.com/science/article/pii/S0168927411001814},
	doi = {10.1016/j.apnum.2011.10.001},
	series = {Third Chilean Workshop on Numerical Analysis of Partial Differential Equations ({WONAPDE} 2010)},
	abstract = {The present paper is dedicated to the application of the pivoted Cholesky decomposition to compute low-rank approximations of dense, positive semi-definite matrices. The resulting truncation error is rigorously controlled in terms of the trace norm. Exponential convergence rates are proved under the assumption that the eigenvalues of the matrix under consideration exhibit a sufficiently fast exponential decay. By numerical experiments it is demonstrated that the pivoted Cholesky decomposition leads to very efficient algorithms to separate the variables of bi-variate functions.},
	pages = {428--440},
	number = {4},
	journaltitle = {Applied Numerical Mathematics},
	shortjournal = {Applied Numerical Mathematics},
	author = {Harbrecht, Helmut and Peters, Michael and Schneider, Reinhold},
	urldate = {2022-03-16},
	date = {2012-04-01},
	langid = {english},
	keywords = {Cholesky decomposition, Low-rank approximation},
}


@article{henderson_deriving_1981,
	title = {On Deriving the Inverse of a Sum of Matrices},
	volume = {23},
	issn = {0036-1445},
	url = {https://www.jstor.org/stable/2029838},
	abstract = {Available expressions are reviewed and new ones derived for the inverse of the sum of two matrices, one of them being nonsingular. Particular attention is given to (A + {UBV})-1, where A is nonsingular and U, B and V may be rectangular; generalized inverses of A + {UBV} are also considered. Several statistical applications are discussed.},
	pages = {53--60},
	number = {1},
	journaltitle = {{SIAM} Review},
	author = {Henderson, H. V. and Searle, S. R.},
	urldate = {2022-03-16},
	date = {1981},
	note = {Publisher: Society for Industrial and Applied Mathematics},
}